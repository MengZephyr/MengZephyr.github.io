---
layout: post
title: Modeling Hair from an RGB-D Camera
description: # Add post description (optional)
img: p3.jpg # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [Publication]
---
<center>Meng Zhang, Pan Wu, Hongzhi Wu, YanLin Wen, Youyi Zhen, Kun Zhou, SIGGRAPH Asia 2018 (Conditionally Accpeted)</center>

## Abstract
Creating complete and realistic 3D hairs to closely match the real-world inputs remains challenging. With the increasing popularity of lightweight depth cameras featured in devices such as iPhone X, Intel RealSense, and DJI drones, additional depth cues are easily acquirable to assist many entertainment applications, for example, the Animated Emoji. In this paper, we introduce a data-driven and fully automatic approach to model the hair geometry and compute a complete strand-level 3D hair model that closely resembles both the fusion model and the hair textures using a single RGB-D camera. Our method heavily exploits the geometric cues offered in the depth channel and leverages exemplars in 3D hair database for high-fidelity hair synthesis. The core of our method is a local-similarity based search and synthesis algorithm that simultaneously reasons about the hair geometry, strands connectivity, strand orientation, and hair structural plausibility. We demonstrate the efficacy of our method using a variety of complex hairstyles and compare our method with prior arts.


## BibTex


